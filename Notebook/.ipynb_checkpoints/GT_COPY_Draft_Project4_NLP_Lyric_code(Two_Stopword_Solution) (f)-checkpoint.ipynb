{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ob5OO-0Dkb41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /Users/adamfreedman/opt/anaconda3/envs/python3.2/lib/python3.6/site-packages (1.8.1)\r\n",
      "Requirement already satisfied: matplotlib in /Users/adamfreedman/opt/anaconda3/envs/python3.2/lib/python3.6/site-packages (from wordcloud) (3.1.3)\r\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/adamfreedman/opt/anaconda3/envs/python3.2/lib/python3.6/site-packages (from wordcloud) (1.18.1)\r\n",
      "Requirement already satisfied: pillow in /Users/adamfreedman/opt/anaconda3/envs/python3.2/lib/python3.6/site-packages (from wordcloud) (8.4.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/adamfreedman/opt/anaconda3/envs/python3.2/lib/python3.6/site-packages (from matplotlib->wordcloud) (1.2.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/adamfreedman/opt/anaconda3/envs/python3.2/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.4.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/adamfreedman/opt/anaconda3/envs/python3.2/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.8.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/adamfreedman/opt/anaconda3/envs/python3.2/lib/python3.6/site-packages (from matplotlib->wordcloud) (0.10.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/adamfreedman/opt/anaconda3/envs/python3.2/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib->wordcloud) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data in the \"../Resources/\" directory.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [16, 10]\n",
    "plt.rcParams['font.size'] = 14\n",
    "width = 0.75\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import string\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "sns.set_palette(sns.color_palette('tab20', 20))\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "from datetime import date, timedelta\n",
    "import operator \n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import spacy #load spacy\n",
    "nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
    "#stops = stopwords.words(\"english\")\n",
    "from tqdm import  tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "print(os.listdir(\"../Resources\"))\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pRciUcFh4dL"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../input/lyrics.csv\",encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwFo9sGdidab"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0lqiQXVjTX2"
   },
   "outputs": [],
   "source": [
    "def get_features(df):    \n",
    "    data['lyric'] = data['lyric'].apply(lambda x:str(x))\n",
    "    data['total_length'] = data['lyric'].apply(len)\n",
    "    data['capitals'] = data['lyric'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    data['caps_vs_length'] = data.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    data['num_words'] = data.lyric.str.count('\\S+')\n",
    "    data['num_unique_words'] = data['lyric'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    data['words_vs_unique'] = data['num_unique_words'] / df['num_words']  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyjI1wk1kHwa"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "y1 = data[data['year'] == 2017]['lyric'].str.len()\n",
    "sns.distplot(y1, label='2017')\n",
    "y2 = data[data['year'] == 2014]['lyric'].str.len()\n",
    "sns.distplot(y2, label='2014')\n",
    "y3 = data[data['year'] == 2012]['lyric'].str.len()\n",
    "sns.distplot(y3, label='2012')\n",
    "y4 = data[data['year'] == 2010]['lyric'].str.len()\n",
    "sns.distplot(y4, label='2010')\n",
    "y5 = data[data['year'] == 2008]['lyric'].str.len()\n",
    "sns.distplot(y5, label='2008')\n",
    "y6 = data[data['year'] == 2006]['lyric'].str.len()\n",
    "sns.distplot(y6, label='2006')\n",
    "plt.title('Year Wise - Lyrics Lenght Distribution (Without Preprocessing)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9pp5L6bkyIR"
   },
   "outputs": [],
   "source": [
    "# lyric features year length etc, basic understanding;\n",
    "train = get_features(data)\n",
    "data_pair = data.filter(['year','total_length','capitals','caps_vs_length','num_words','num_unique_words','words_vs_unique'],axis=1)\n",
    "\n",
    "#Lyric Cleaning solution 1, Contraction mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8D5oprMflBp-"
   },
   "outputs": [],
   "source": [
    "contraction_mapping_1 = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \n",
    "                       \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n",
    "                       \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n",
    "                       \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n",
    "                       \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
    "                       \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
    "                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                       \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n",
    "                       \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n",
    "                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n",
    "                       \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n",
    "                       \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                       \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n",
    "                       \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
    "                       \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n",
    "                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
    "                       \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "                       \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \n",
    "                       \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \n",
    "                       \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                       \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                       \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                       \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                       \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" ,\n",
    "                       \"Isn't\":\"is not\", \"\\u200b\":\"\", \"It's\": \"it is\",\"I'm\": \"I am\",\"don't\":\"do not\",\"did't\":\"did not\",\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                       \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                       \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                       \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                       \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                       \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                       \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                       \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n",
    "                       \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n",
    "                       \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
    "                       \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n",
    "                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n",
    "                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
    "                       \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \n",
    "                       \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\",\n",
    "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVDzMcINlIQI"
   },
   "outputs": [],
   "source": [
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IfAloPAlNh6"
   },
   "outputs": [],
   "source": [
    "def get_features(df):    \n",
    "    data['Clean_Lyrics'] = data['Clean_Lyrics'].apply(lambda x:str(x))\n",
    "    data['total_length'] = data['Clean_Lyrics'].apply(len)\n",
    "    data['capitals'] = data['Clean_Lyrics'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    data['caps_vs_length'] = data.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    data['num_words'] = data.lyric.str.count('\\S+')\n",
    "    data['num_unique_words'] = data['Clean_Lyrics'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    data['words_vs_unique'] = data['num_unique_words'] / df['num_words']  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebV5yZU-lUd6"
   },
   "outputs": [],
   "source": [
    "data['Clean_Lyrics'] = data['lyric'].apply(lambda x: clean_contractions(x, contraction_mapping_1))\n",
    "#Stopwords\n",
    "data['Clean_Lyrics'] = data['Clean_Lyrics'].apply(lambda x: ' '.join([word for word in x.split() if word not in (STOPWORDS)]))\n",
    "#Re-calculate the features\n",
    "train = get_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poVuDkErpsG-"
   },
   "outputs": [],
   "source": [
    "#Lyric Clean solution 2(Class demonstration Pyspark.ml ), create the stop words \n",
    "\n",
    "\n",
    "import os\n",
    "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.0.3'\n",
    "#spark_version = 'spark-3.<enter version>'\n",
    "spark_version = 'spark-3.2.1'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diO5wvUPqLU0"
   },
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"StopWords\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07qdeILFqeHn"
   },
   "outputs": [],
   "source": [
    "# import stopwords library\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9ziYZOuqmgA"
   },
   "outputs": [],
   "source": [
    "# Create DataFrame / change this to pyspark read in dataframe\n",
    "\n",
    "      # sentenceData = spark.createDataFrame([\n",
    "      #     (0, [\"Big\", \"data\", \"is\", \"super\", \"powerful\"]),\n",
    "      #     (1, [\"This\", \"is\", \"going\", \"to\", \"be\", \"epic\"])\n",
    "      # ], [\"id\", \"raw\"])\n",
    "\n",
    "      # sentenceData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfrzuma4q8cm"
   },
   "outputs": [],
   "source": [
    "# Instantiate Remover\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvKIWsgdrDWk"
   },
   "outputs": [],
   "source": [
    "# Transform and show data\n",
    "remover.transform(Data).show(truncate=False)# lyric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJZyGfXbsDXv"
   },
   "outputs": [],
   "source": [
    "# Transform and show data\n",
    "remover.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UokijakZlZEo"
   },
   "outputs": [],
   "source": [
    "#continue code with solution 1 or modified solution 2\n",
    "data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrbNNQTtlkMO"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "y1 = data[data['year'] == 2017]['Clean_Lyrics'].str.len()\n",
    "sns.distplot(y1, label='2017')\n",
    "y2 = data[data['year'] == 2014]['Clean_Lyrics'].str.len()\n",
    "sns.distplot(y2, label='2014')\n",
    "y3 = data[data['year'] == 2012]['Clean_Lyrics'].str.len()\n",
    "sns.distplot(y3, label='2012')\n",
    "y4 = data[data['year'] == 2010]['Clean_Lyrics'].str.len()\n",
    "sns.distplot(y4, label='2010')\n",
    "y5 = data[data['year'] == 2008]['Clean_Lyrics'].str.len()\n",
    "sns.distplot(y5, label='2008')\n",
    "y6 = data[data['year'] == 2006]['Clean_Lyrics'].str.len()\n",
    "sns.distplot(y6, label='2006')\n",
    "plt.title('Year Wise - Lyrics Lenght Distribution (After Preprocessing)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "siB8Ln_jlqgp"
   },
   "outputs": [],
   "source": [
    "data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxhh6eDWl_ji"
   },
   "outputs": [],
   "source": [
    "def ngram_extractor(text, n_gram):\n",
    "    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n",
    "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "# Function to generate a dataframe with n_gram and top max_row frequencies\n",
    "def generate_ngrams(df, col, n_gram, max_row):\n",
    "    temp_dict = defaultdict(int)\n",
    "    for question in df[col]:\n",
    "        for word in ngram_extractor(question, n_gram):\n",
    "            temp_dict[word] += 1\n",
    "    temp_df = pd.DataFrame(sorted(temp_dict.items(), key=lambda x: x[1])[::-1]).head(max_row)\n",
    "    temp_df.columns = [\"word\", \"wordcount\"]\n",
    "    return temp_df\n",
    "\n",
    "def comparison_plot(df_1,df_2,col_1,col_2, space):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "    \n",
    "    sns.barplot(x=col_2, y=col_1, data=df_1, ax=ax[0], color=\"skyblue\")\n",
    "    sns.barplot(x=col_2, y=col_1, data=df_2, ax=ax[1], color=\"skyblue\")\n",
    "\n",
    "    ax[0].set_xlabel('Word count', size=14, color=\"green\")\n",
    "    ax[0].set_ylabel('Words', size=18, color=\"green\")\n",
    "    ax[0].set_title('Top words in 2017 Lyrics', size=18, color=\"green\")\n",
    "\n",
    "    ax[1].set_xlabel('Word count', size=14, color=\"green\")\n",
    "    ax[1].set_ylabel('Words', size=18, color=\"green\")\n",
    "    ax[1].set_title('Top words in 2008 Lyrics', size=18, color=\"green\")\n",
    "\n",
    "    fig.subplots_adjust(wspace=space)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aARg6sTDmIoy"
   },
   "outputs": [],
   "source": [
    "# Ngram Lyrics Anaysis 2017 vs /2008 word just 1\n",
    "Lyrics_2017 = generate_ngrams(train[train[\"year\"]==2017], 'Clean_Lyrics', 1, 10)\n",
    "Lyrics_2008 = generate_ngrams(data[data[\"year\"]==2008], 'Clean_Lyrics', 1, 10)\n",
    "comparison_plot(Lyrics_2017,Lyrics_2008,'word','wordcount', 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WSH3Xi2mdAt"
   },
   "outputs": [],
   "source": [
    "#Bigram Lyrics Anaysis 2017 vs 2008 / two combined word\n",
    "Lyrics_2017 = generate_ngrams(train[train[\"year\"]==2017], 'Clean_Lyrics', 2, 10)\n",
    "Lyrics_2008 = generate_ngrams(data[data[\"year\"]==2008], 'Clean_Lyrics', 2, 10)\n",
    "comparison_plot(Lyrics_2017,Lyrics_2008,'word','wordcount', 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtV7ksH1mzwX"
   },
   "outputs": [],
   "source": [
    "# Trigram Lyrics Anaysis 2017 vs 2008\n",
    "Lyrics_2017 = generate_ngrams(train[train[\"year\"]==2017], 'Clean_Lyrics', 3, 10)\n",
    "Lyrics_2008 = generate_ngrams(data[data[\"year\"]==2008], 'Clean_Lyrics', 3, 10)\n",
    "comparison_plot(Lyrics_2017,Lyrics_2008,'word','wordcount', 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "Z3SvRZGRnKnl",
    "outputId": "815d0d90-521f-4430-cf65-cf855dac8034"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-347c85041004>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    data['parsed'] = data.Clean_Lyrics.apply(nlp)\u001b[0m\n\u001b[0m                                                 \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "#scatter text\n",
    "'''\n",
    "Scattertext\n",
    "open source tool for visualizing linguistic variation between document categories in a language-independent way. \n",
    "The tool presents a scatterplot, where each axis corresponds to the rank-frequency a term occurs in a category of documents. \n",
    "Through a tie-breaking strategy, the tool is able to display thousands of visible term-representing points and find space to legibly label hundreds of them. \n",
    "Scattertext also lends itself to a query-based visualization of how the use of terms with similar embeddings differs between document categories, as well as a visualization for comparing the importance scores of bag-of-words features to univariate metrics.\n",
    "'''\n",
    "import scattertext as st\n",
    "nlp = spacy.load('en',disable_pipes=[\"tagger\",\"ner\"])\n",
    "data['parsed'] = data.Clean_Lyrics.apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Dhxz5pznR63"
   },
   "outputs": [],
   "source": [
    "corpus = st.CorpusFromParsedDocuments(data,\n",
    "                             category_col='album',\n",
    "                             parsed_col='parsed').build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3IEWW81na3s"
   },
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(corpus,\n",
    "          category='reputation',\n",
    "          category_name='reputation',\n",
    "          not_category_name='1989',\n",
    "          width_in_pixels=600,\n",
    "          minimum_term_frequency=5,\n",
    "          term_significance = st.LogOddsRatioUninformativeDirichletPrior(),\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00735WKjnmCj"
   },
   "outputs": [],
   "source": [
    "filename = \"reputation-vs-1989.html\"\n",
    "open(filename, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=filename, width = 800, height=700)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "GT_COPY_Draft_Project4_NLP_Lyric_code(Two_Stopword_Solution).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python3.2",
   "language": "python",
   "name": "python3.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
